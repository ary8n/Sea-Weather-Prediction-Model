{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvLSZrsK4onO",
        "outputId": "d24556dd-b10c-4f0f-c828-1978071a6b52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded potential temperature.csv successfully! Shape: (17473, 1)\n",
            "ğŸ” Missing Values Before Filling: 8737\n",
            "ğŸ” Missing Values After Filling: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0170 - val_loss: 0.0012\n",
            "Epoch 2/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0042 - val_loss: 0.0012\n",
            "Epoch 3/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0030 - val_loss: 8.2392e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0025 - val_loss: 0.0012\n",
            "Epoch 5/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0024 - val_loss: 7.0604e-04\n",
            "Epoch 6/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0024 - val_loss: 6.7703e-04\n",
            "Epoch 7/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0023 - val_loss: 6.5660e-04\n",
            "Epoch 8/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0023 - val_loss: 6.9634e-04\n",
            "Epoch 9/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0025 - val_loss: 8.0792e-04\n",
            "Epoch 10/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0024 - val_loss: 6.5824e-04\n",
            "Epoch 11/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0025 - val_loss: 6.6666e-04\n",
            "Epoch 12/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0024 - val_loss: 8.8305e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0023 - val_loss: 6.2033e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0021 - val_loss: 8.2032e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0020 - val_loss: 6.5265e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0021 - val_loss: 8.5219e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0022 - val_loss: 8.1941e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0021 - val_loss: 0.0012\n",
            "Epoch 19/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - loss: 0.0019 - val_loss: 8.9199e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0019 - val_loss: 6.9354e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0019 - val_loss: 6.2721e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0021 - val_loss: 6.2521e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - loss: 0.0019 - val_loss: 7.0482e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0019 - val_loss: 7.9275e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0019 - val_loss: 7.3558e-04\n",
            "Epoch 26/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0019 - val_loss: 7.6199e-04\n",
            "Epoch 27/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0020 - val_loss: 6.3491e-04\n",
            "Epoch 28/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0016 - val_loss: 6.5863e-04\n",
            "Epoch 29/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0017 - val_loss: 6.4546e-04\n",
            "Epoch 30/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0017 - val_loss: 6.4540e-04\n",
            "Epoch 31/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - loss: 0.0015 - val_loss: 6.3543e-04\n",
            "Epoch 32/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0019 - val_loss: 9.7310e-04\n",
            "Epoch 33/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0017 - val_loss: 7.7292e-04\n",
            "Epoch 34/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0016 - val_loss: 8.1481e-04\n",
            "Epoch 35/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0016 - val_loss: 7.0079e-04\n",
            "Epoch 36/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0015 - val_loss: 7.4608e-04\n",
            "Epoch 37/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 7.0169e-04\n",
            "Epoch 38/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0016 - val_loss: 9.4331e-04\n",
            "Epoch 39/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 6.4620e-04\n",
            "Epoch 40/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 9.2097e-04\n",
            "Epoch 41/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 6.6882e-04\n",
            "Epoch 42/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0015 - val_loss: 6.7851e-04\n",
            "Epoch 43/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - loss: 0.0014 - val_loss: 8.8006e-04\n",
            "Epoch 44/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0014 - val_loss: 6.2879e-04\n",
            "Epoch 45/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 6.8227e-04\n",
            "Epoch 46/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 7.5010e-04\n",
            "Epoch 47/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 7.5680e-04\n",
            "Epoch 48/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0012 - val_loss: 6.6440e-04\n",
            "Epoch 49/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - loss: 0.0013 - val_loss: 8.7704e-04\n",
            "Epoch 50/50\n",
            "\u001b[1m764/764\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0013 - val_loss: 6.7269e-04\n",
            "\u001b[1m164/164\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1032e-04\n",
            "âœ… Test Loss (MSE): 0.0007\n",
            "\u001b[1m164/164\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
            "ğŸ“Š RÂ² Score: 0.9867\n",
            "ğŸ“‰ Mean Absolute Error (MAE): 0.0171\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "ğŸ”® Predicted Future Value: [26.53129]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"potential temperature.csv\"\n",
        "\n",
        "if os.path.exists(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, delimiter=\",\", encoding=\"utf-8\", on_bad_lines=\"skip\", engine=\"python\", skiprows=8)  # Skip metadata\n",
        "\n",
        "        # Convert all columns (except datetime) to numeric, forcing errors to NaN\n",
        "        for col in df.columns:\n",
        "            if col not in [\"DATETIME\", \"TIME\"]:  # Exclude time-related columns\n",
        "                df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
        "\n",
        "        # Replace -1.E+34 (error values) with NaN\n",
        "        df.replace([-1e+34, \"-1.E+34\", -1.0e+34, \"-1.000000e+34\"], np.nan, inplace=True)\n",
        "\n",
        "        print(f\"âœ… Loaded {file_path} successfully! Shape: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error loading {file_path}: {e}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ Warning: {file_path} not found!\")\n",
        "    exit()\n",
        "\n",
        "# Check for missing values and fill them\n",
        "missing_before = df.isnull().sum().sum()\n",
        "print(f\"ğŸ” Missing Values Before Filling: {missing_before}\")\n",
        "\n",
        "# Fill missing values using bfill, ffill, and interpolation\n",
        "df.bfill(axis=0, inplace=True)  # Backfill\n",
        "df.ffill(axis=0, inplace=True)  # Forward fill\n",
        "df.interpolate(method='linear', axis=0, inplace=True)  # Linear interpolation\n",
        "\n",
        "missing_after = df.isnull().sum().sum()\n",
        "print(f\"ğŸ” Missing Values After Filling: {missing_after}\")\n",
        "\n",
        "# Select numeric columns\n",
        "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "\n",
        "if not numeric_cols.empty:\n",
        "    # Scale data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(df[numeric_cols])\n",
        "\n",
        "    # Reshape data for LSTM\n",
        "    sequence_length = 10\n",
        "    X = []\n",
        "    y = []\n",
        "    for i in range(sequence_length, len(scaled_data)):\n",
        "        X.append(scaled_data[i-sequence_length:i])\n",
        "        y.append(scaled_data[i])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    # Split data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)\n",
        "\n",
        "    # Reshape for LSTM input\n",
        "    X_train = X_train.reshape(X_train.shape[0], sequence_length, scaled_data.shape[1])\n",
        "    X_test = X_test.reshape(X_test.shape[0], sequence_length, scaled_data.shape[1])\n",
        "\n",
        "    # Build LSTM model\n",
        "    model = Sequential([\n",
        "        LSTM(units=50, return_sequences=True, input_shape=(sequence_length, scaled_data.shape[1])),\n",
        "        LSTM(units=50),\n",
        "        Dense(scaled_data.shape[1])\n",
        "    ])\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
        "\n",
        "    # Evaluate model\n",
        "    loss = model.evaluate(X_test, y_test)\n",
        "    print(f\"âœ… Test Loss (MSE): {loss:.4f}\")\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Compute RÂ² Score and MAE\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"ğŸ“Š RÂ² Score: {r2:.4f}\")\n",
        "    print(f\"ğŸ“‰ Mean Absolute Error (MAE): {mae:.4f}\")\n",
        "\n",
        "    # Predict future value\n",
        "    last_sequence = scaled_data[-sequence_length:]\n",
        "    last_sequence = last_sequence.reshape(1, sequence_length, scaled_data.shape[1])\n",
        "    future_value = model.predict(last_sequence)\n",
        "    future_value = scaler.inverse_transform(future_value)[0]  # Inverse transform to original scale\n",
        "\n",
        "    print(f\"ğŸ”® Predicted Future Value: {future_value}\")\n",
        "else:\n",
        "    print(f\"âš ï¸ No numeric columns found in {file_path}.\")\n"
      ]
    }
  ]
}