import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# List of CSV files
csv_files = [
    "dry blub temperature.csv",
    "potential temperature.csv",
    "sea surface temperature.csv",
    "surface heigh.csv",
    "surface_height.csv",
    "vertical velocity at t points.csv"
]

# Dictionary to store DataFrames
dfs = {}

# Load datasets
for file in csv_files:
    if os.path.exists(file):
        try:
            df = pd.read_csv(file, delimiter=",", encoding="utf-8", on_bad_lines="skip", engine="python", skiprows=8)  # Skip metadata

            # Convert all columns (except datetime) to numeric, forcing errors to NaN
            for col in df.columns:
                if col not in ["DATETIME", "TIME"]:  # Exclude time-related columns
                    df[col] = pd.to_numeric(df[col], errors="coerce")

            # Replace -1.E+34 (error values) with NaN
            df.replace([-1e+34, "-1.E+34", -1.0e+34, "-1.000000e+34"], np.nan, inplace=True)

            dfs[file] = df
            print(f"‚úÖ Loaded {file} successfully! Shape: {df.shape}")
        except Exception as e:
            print(f"‚ùå Error loading {file}: {e}")
    else:
        print(f"‚ö†Ô∏è Warning: {file} not found!")

# Function to perform LSTM on each dataset
def perform_lstm(dfs):
    for file, df in dfs.items():
        print(f"\nüìÇ **Performing LSTM on {file}**")
        
        # Select numeric columns
        numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns
        
        if not numeric_cols.empty:
            # Scale data
            scaler = MinMaxScaler()
            scaled_data = scaler.fit_transform(df[numeric_cols])
            
            # Reshape data for LSTM
            sequence_length = 10
            X = []
            y = []
            for i in range(sequence_length, len(scaled_data)):
                X.append(scaled_data[i-sequence_length:i])
                y.append(scaled_data[i])
            
            X = np.array(X)
            y = np.array(y)
            
            # Split data into training and testing sets
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=False)
            
            # Reshape for LSTM input
            X_train = X_train.reshape(X_train.shape[0], sequence_length, scaled_data.shape[1])
            X_test = X_test.reshape(X_test.shape[0], sequence_length, scaled_data.shape[1])
            
            # Build LSTM model
            model = Sequential([
                LSTM(units=50, return_sequences=True, input_shape=(sequence_length, scaled_data.shape[1])),
                LSTM(units=50),
                Dense(scaled_data.shape[1])
            ])
            
            # Compile model
            model.compile(optimizer='adam', loss='mse')
            
            # Train model
            model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))
            
            # Evaluate model
            loss = model.evaluate(X_test, y_test)
            print(f"‚úÖ Test Loss: {loss}")
        else:
            print(f"‚ö†Ô∏è No numeric columns found in {file}.")

# Perform LSTM on each dataset
perform_lstm(dfs)
